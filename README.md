# Pipeline de Dados E-commerce para AnÃ¡lise de Vendas

Este projeto implementa um pipeline de ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga (ETL) para processar dados de vendas de produtos de um e-commerce. Os dados sÃ£o extraÃ­dos de uma API, armazenados em um formato bruto no MongoDB, transformados, e entÃ£o carregados em uma tabela de staging no PostgreSQL. Subsequentemente, um Data Warehouse com um modelo Star Schema Ã© populado a partir da tabela de staging para permitir anÃ¡lises de Business Intelligence.

Este projeto foi desenvolvido como um exercÃ­cio prÃ¡tico e peÃ§a de portfÃ³lio, demonstrando habilidades em engenharia de dados, Python, SQL, MongoDB, PostgreSQL, Docker e modelagem de dados.

```mermaid
flowchart TD
    A["API de Produtos Externa[ðŸŒ]"] --> B["Script Python de ExtraÃ§Ã£o[ðŸ]"]
    B --> C[("Dados Brutos")]
    C --> D[("MongoDB [ðŸƒ] Armazenamento de Dados Brutos")]
    D --> E["Script Python de TransformaÃ§Ã£o [ðŸ]"]
    F["Regras de TransformaÃ§Ã£o - Limpeza - NormalizaÃ§Ã£o - ConversÃ£o"] -.-> E
    E --> G[("Dados Transformados")]
    G --> H[("PostgreSQL [ðŸ˜] Tabela de Staging")]
    H --> I["Scripts SQL [ðŸ“Š]"]
    I --> J["Data Warehouse Star Schema [ðŸ˜ PostgreSQL]"]
    J --> K["Tabelas de DimensÃ£o - Produtos - Clientes - Tempo - LocalizaÃ§Ã£o"]
    J --> L["Tabela Fato - Vendas"]
    K --> M["AnÃ¡lise de BI [ðŸ“ˆ]"]
    L --> M
    
    classDef api fill:#f9f9ff,stroke:#333,stroke-width:2px
    classDef python fill:#e6f3ff,stroke:#333,stroke-width:2px
    classDef data fill:#f9f6e5,stroke:#333,stroke-width:2px
    classDef mongodb fill:#e6ffe6,stroke:#333,stroke-width:2px
    classDef postgres fill:#ffe6e6,stroke:#333,stroke-width:2px
    classDef sql fill:#f2e6ff,stroke:#333,stroke-width:2px
    classDef bi fill:#ffe6f2,stroke:#333,stroke-width:2px
    classDef warehouse fill:#e6ffff,stroke:#333,stroke-width:2px
    
    class A api
    class B,E python
    class C,G data
    class D mongodb
    class H,J,K,L postgres
    class I sql
    class M bi
    class F data
```

## Funcionalidades

*   **ExtraÃ§Ã£o de Dados:** Coleta dados de produtos de uma API REST.
*   **Armazenamento Bruto:** Persiste os dados brutos extraÃ­dos no MongoDB.
*   **TransformaÃ§Ã£o de Dados:** Limpa, normaliza, converte e enriquece os dados.
*   **Carga em Staging:** Carrega dados transformados em uma tabela de staging no PostgreSQL.
*   **CriaÃ§Ã£o de Data Warehouse:** Define e cria o esquema de um Star Schema no PostgreSQL.
*   **PopulaÃ§Ã£o do Data Warehouse:** Popula as dimensÃµes e fatos a partir da staging.
*   **Consultas AnalÃ­ticas:** Fornece exemplos de queries SQL para anÃ¡lise.
*   **ConfiguraÃ§Ã£o Centralizada:** Usa `core/config.py` para gerenciar configuraÃ§Ãµes.
*   **Modularidade:** CÃ³digo organizado em mÃ³dulos com responsabilidades claras.
*   **Gerenciamento de Ambiente Dockerizado:** Utiliza Docker e Docker Compose para os serviÃ§os de banco de dados, gerenciados por um `Makefile`.

## Estrutura do Projeto

```
ecommerce-data-pipeline/
â”œâ”€â”€ .env.template           # Template de configuraÃ§Ãµes de ambiente
â”œâ”€â”€ .docker/
â”‚   â””â”€â”€ compose.yml         # Arquivo Docker Compose para serviÃ§os de BD
â”œâ”€â”€ Makefile                # Comandos para gerenciar o ambiente Docker
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/               # MÃ³dulos centrais (config, DB managers)
â”‚   â”œâ”€â”€ pipeline/           # MÃ³dulos das etapas do ETL
â”‚   â”œâ”€â”€ sql/                # Scripts SQL (schema, populate, queries)
â”‚   â””â”€â”€ main.py             # Orquestrador principal do pipeline
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â””â”€â”€ README.md               # Este arquivo
```

## Tecnologias Utilizadas

*   **Python 3.12+**
*   **MongoDB:** Banco de dados NoSQL (dados brutos).
*   **PostgreSQL:** Banco de dados SQL (staging e Data Warehouse).
*   **Docker & Docker Compose:** Para containerizaÃ§Ã£o e gerenciamento dos serviÃ§os de banco de dados.
*   **Make:** Para simplificar os comandos Docker Compose.
*   **Bibliotecas Python Principais:**
    *   `requests` (ExtraÃ§Ã£o da API)
    *   `pymongo` (Driver MongoDB)
    *   `psycopg2-binary` (Driver PostgreSQL)
    *   `python-dotenv` (VariÃ¡veis de ambiente)
    *   `logging` (Logs da aplicaÃ§Ã£o)
*   **SQL:** Para DDL, DML e consultas analÃ­ticas.

## ConfiguraÃ§Ã£o do Ambiente

1.  **PrÃ©-requisitos:**
    *   Git
    *   Python 3.12+
    *   Pip
    *   Docker Engine
    *   Docker Compose (geralmente vem com o Docker Desktop ou pode ser instalado separadamente)
    *   Make (comum em sistemas Linux/macOS; no Windows, pode ser necessÃ¡rio instalar via Chocolatey, MSYS2, ou usar WSL)

2.  **Clone o RepositÃ³rio:**
    ```bash
    git clone https://github.com/esscova/ecommerce-data-pipeline.git
    cd ecommerce-data-pipeline
    ```

3.  **Crie e Ative um Ambiente Virtual Python (para rodar `main.py` localmente):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # Linux/macOS
    # .venv\Scripts\activate   # Windows
    ```

4.  **Instale as DependÃªncias Python:**
    ```bash
    pip install -r requirements.txt
    ```

## Gerenciamento do Ambiente com Makefile

Este projeto utiliza um `Makefile` para simplificar a interaÃ§Ã£o com os contÃªineres Docker Compose. Os principais comandos sÃ£o:

*   **Iniciar os contÃªineres dos bancos de dados:**
    ```bash
    make up
    ```
    Isso utiliza o arquivo `.docker/compose.yml` e as variÃ¡veis do `.env`.

*   **Parar e remover os contÃªineres:**
    ```bash
    make down
    ```

*   **Acessar o shell de um contÃªiner:**
    ```bash
    make shell-mongo    # Acessa o shell bash do contÃªiner MongoDB
    make shell-postgres # Acessa o shell bash do contÃªiner PostgreSQL
    ```

*   **Conectar diretamente aos bancos de dados:**
    ```bash
    make connect-mongo    # Abre o mongosh conectado ao MongoDB
    make connect-postgres # Abre o psql conectado ao PostgreSQL (usuÃ¡rio 'admin', banco 'analytics')
    ```

*   **Visualizar logs dos contÃªineres:**
    ```bash
    make logs-mongo
    make logs-postgres
    ```

*   **Limpar volumes de dados (ATENÃ‡ÃƒO: ISSO APAGA OS DADOS PERSISTIDOS):**
    ```bash
    make clean-mongo    # Para e remove o contÃªiner MongoDB e seu volume de dados
    make clean-postgres # Para e remove o contÃªiner PostgreSQL e seu volume de dados
    make clean          # Para e remove TODOS os contÃªineres e TODOS os volumes definidos no compose
    ```

*   **Ver todos os comandos disponÃ­veis:**
    ```bash
    make help
    ```

## Como Executar o Pipeline ETL

1.  **Inicie os ServiÃ§os de Banco de Dados:**
    Certifique-se de que suas instÃ¢ncias do MongoDB e PostgreSQL estejam rodando. Se estiver usando o Docker Compose gerenciado pelo Makefile:
    ```bash
    make up
    ```
    Aguarde alguns segundos para que os bancos de dados iniciem completamente.

2.  **Execute o Orquestrador Principal:**
    Com seu ambiente virtual Python ativado e as dependÃªncias instaladas:
    ```bash
    python src/main.py
    ```

3.  **Fluxo de ExecuÃ§Ã£o:**
    *   **Fase 0:** CriaÃ§Ã£o/verificaÃ§Ã£o do esquema do Data Warehouse no PostgreSQL.
    *   **Etapa 1:** ExtraÃ§Ã£o de dados da API.
    *   **Etapa 2:** Carga dos dados brutos no MongoDB (a coleÃ§Ã£o Ã© limpa antes).
    *   **Etapa 3:** ExtraÃ§Ã£o dos dados brutos do MongoDB.
    *   **Etapa 4:** TransformaÃ§Ã£o dos dados.
    *   **Etapa 5:** Carga dos dados transformados na tabela de staging do PostgreSQL (a tabela Ã© truncada antes).
    *   **Etapa 6:** PopulaÃ§Ã£o das tabelas de dimensÃ£o e fato a partir da staging.

4.  **Verifique os Logs:** Acompanhe a saÃ­da no console.

## Consultando o Data Warehouse

ApÃ³s a execuÃ§Ã£o bem-sucedida do pipeline, conecte-se ao seu banco de dados PostgreSQL (vocÃª pode usar `make connect-postgres`) e execute as queries localizadas em `src/sql/queries/` para analisar os dados.

## Estrutura do Data Warehouse (Star Schema)

*   **Tabela Fato:** `FATO_VENDAS`
*   **Tabelas de DimensÃ£o:** `DIM_PRODUTO`, `DIM_VENDEDOR`, `DIM_LOCAL`, `DIM_TEMPO`, `DIM_PAGAMENTO`.
Consulte `src/sql/schema/` para detalhes.

## Autor
[Wellington M Santos](https://www.linkedin.com/in/wellington-moreira-santos/)
